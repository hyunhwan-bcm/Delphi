{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delphi Model Evaluation on Test Set\n",
    "\n",
    "This notebook evaluates the trained Delphi model on the test dataset.\n",
    "\n",
    "**Metrics:**\n",
    "- Cross-Entropy Loss (CE): Measures accuracy of next event prediction\n",
    "- Time-to-Event Loss (DT): Measures accuracy of time prediction\n",
    "- Perplexity: exp(CE loss), measures model uncertainty\n",
    "- Combined Loss: CE + DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from model import Delphi, DelphiConfig\n",
    "from utils import get_p2i, get_batch\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "checkpoint_path = 'mc-med-gpt/ckpt_10000.pt'  # or 'mc-med-gpt/ckpt.pt'\n",
    "data_dir = 'data/mc-med'\n",
    "device = 'mps'  # 'cuda', 'mps', or 'cpu'\n",
    "batch_size = 64\n",
    "block_size = 128  # Should match training config\n",
    "no_event_token_rate = 5\n",
    "eval_iters = 100  # Number of batches to evaluate (increase for more accurate results)\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Checkpoint and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Extract model configuration\n",
    "model_args = checkpoint['model_args']\n",
    "print(f\"\\nModel configuration:\")\n",
    "for k, v in model_args.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Print training info\n",
    "print(f\"\\nTraining info:\")\n",
    "print(f\"  Iteration: {checkpoint['iter_num']}\")\n",
    "print(f\"  Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "\n",
    "# Create model\n",
    "gptconf = DelphiConfig(**model_args)\n",
    "model = Delphi(gptconf)\n",
    "\n",
    "# Load weights\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k, v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"\\nModel loaded successfully and moved to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_data = np.memmap(os.path.join(data_dir, 'test.bin'), dtype=np.uint32, mode='r').reshape(-1, 3)\n",
    "test_p2i = get_p2i(test_data)\n",
    "\n",
    "print(f\"Test dataset:\")\n",
    "print(f\"  Total events: {len(test_data):,}\")\n",
    "print(f\"  Total patients: {len(test_p2i):,}\")\n",
    "print(f\"  Avg events per patient: {len(test_data) / len(test_p2i):.2f}\")\n",
    "\n",
    "# Load labels if available\n",
    "labels_path = os.path.join(data_dir, 'labels.csv')\n",
    "if os.path.exists(labels_path):\n",
    "    labels_df = pd.read_csv(labels_path)\n",
    "    print(f\"\\nLoaded {len(labels_df)} event labels\")\n",
    "else:\n",
    "    labels_df = None\n",
    "    print(\"\\nNo labels file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_test_set(model, test_data, test_p2i, eval_iters, batch_size, \n",
    "                      block_size, device, no_event_token_rate):\n",
    "    \"\"\"\n",
    "    Evaluate model on test set following the same approach as BERT evaluation.\n",
    "    Returns detailed metrics including per-batch losses.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Storage for batch-level metrics\n",
    "    batch_losses_ce = []\n",
    "    batch_losses_dt = []\n",
    "    batch_losses_total = []\n",
    "    \n",
    "    print(f\"Evaluating on {eval_iters} batches...\")\n",
    "    for k in tqdm(range(eval_iters)):\n",
    "        # Sample random batch\n",
    "        ix = torch.randint(len(test_p2i), (batch_size,))\n",
    "        X, A, Y, B = get_batch(ix, test_data, test_p2i, \n",
    "                              block_size=block_size,\n",
    "                              device=device, \n",
    "                              select='left',\n",
    "                              no_event_token_rate=no_event_token_rate, \n",
    "                              cut_batch=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss, _ = model(X, A, Y, B, validation_loss_mode=True)\n",
    "        \n",
    "        # Store losses\n",
    "        batch_losses_ce.append(loss['loss_ce'].item())\n",
    "        batch_losses_dt.append(loss['loss_dt'].item())\n",
    "        batch_losses_total.append(loss['loss_ce'].item() + loss['loss_dt'].item())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    batch_losses_ce = np.array(batch_losses_ce)\n",
    "    batch_losses_dt = np.array(batch_losses_dt)\n",
    "    batch_losses_total = np.array(batch_losses_total)\n",
    "    \n",
    "    # Compute statistics\n",
    "    results = {\n",
    "        'loss_ce': {\n",
    "            'mean': batch_losses_ce.mean(),\n",
    "            'std': batch_losses_ce.std(),\n",
    "            'min': batch_losses_ce.min(),\n",
    "            'max': batch_losses_ce.max(),\n",
    "            'median': np.median(batch_losses_ce),\n",
    "        },\n",
    "        'loss_dt': {\n",
    "            'mean': batch_losses_dt.mean(),\n",
    "            'std': batch_losses_dt.std(),\n",
    "            'min': batch_losses_dt.min(),\n",
    "            'max': batch_losses_dt.max(),\n",
    "            'median': np.median(batch_losses_dt),\n",
    "        },\n",
    "        'loss_total': {\n",
    "            'mean': batch_losses_total.mean(),\n",
    "            'std': batch_losses_total.std(),\n",
    "            'min': batch_losses_total.min(),\n",
    "            'max': batch_losses_total.max(),\n",
    "            'median': np.median(batch_losses_total),\n",
    "        },\n",
    "        'perplexity': np.exp(batch_losses_ce.mean()),\n",
    "        'batch_losses_ce': batch_losses_ce,\n",
    "        'batch_losses_dt': batch_losses_dt,\n",
    "        'batch_losses_total': batch_losses_total,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_test_set(\n",
    "    model=model,\n",
    "    test_data=test_data,\n",
    "    test_p2i=test_p2i,\n",
    "    eval_iters=eval_iters,\n",
    "    batch_size=batch_size,\n",
    "    block_size=block_size,\n",
    "    device=device,\n",
    "    no_event_token_rate=no_event_token_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(\"=\"*70)\n",
    "print(\"TEST SET EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCheckpoint: {checkpoint_path}\")\n",
    "print(f\"Training iteration: {checkpoint['iter_num']}\")\n",
    "print(f\"Test batches evaluated: {eval_iters}\")\n",
    "print(f\"Total test samples evaluated: ~{eval_iters * batch_size:,}\")\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "print(\"\\nCROSS-ENTROPY LOSS (Next Event Prediction):\")\n",
    "print(f\"  Mean:   {results['loss_ce']['mean']:.4f} ± {results['loss_ce']['std']:.4f}\")\n",
    "print(f\"  Median: {results['loss_ce']['median']:.4f}\")\n",
    "print(f\"  Range:  [{results['loss_ce']['min']:.4f}, {results['loss_ce']['max']:.4f}]\")\n",
    "\n",
    "print(\"\\nTIME-TO-EVENT LOSS:\")\n",
    "print(f\"  Mean:   {results['loss_dt']['mean']:.4f} ± {results['loss_dt']['std']:.4f}\")\n",
    "print(f\"  Median: {results['loss_dt']['median']:.4f}\")\n",
    "print(f\"  Range:  [{results['loss_dt']['min']:.4f}, {results['loss_dt']['max']:.4f}]\")\n",
    "\n",
    "print(\"\\nCOMBINED LOSS (CE + DT):\")\n",
    "print(f\"  Mean:   {results['loss_total']['mean']:.4f} ± {results['loss_total']['std']:.4f}\")\n",
    "print(f\"  Median: {results['loss_total']['median']:.4f}\")\n",
    "print(f\"  Range:  [{results['loss_total']['min']:.4f}, {results['loss_total']['max']:.4f}]\")\n",
    "\n",
    "print(\"\\nPERPLEXITY (exp of CE loss):\")\n",
    "print(f\"  {results['perplexity']:.2f}\")\n",
    "print(\"  (Lower is better - measures model uncertainty)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# CE Loss\n",
    "axes[0].hist(results['batch_losses_ce'], bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0].axvline(results['loss_ce']['mean'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {results['loss_ce']['mean']:.4f}\")\n",
    "axes[0].axvline(results['loss_ce']['median'], color='green', linestyle='--', linewidth=2, label=f\"Median: {results['loss_ce']['median']:.4f}\")\n",
    "axes[0].set_xlabel('Cross-Entropy Loss')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of CE Loss Across Batches')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# DT Loss\n",
    "axes[1].hist(results['batch_losses_dt'], bins=30, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[1].axvline(results['loss_dt']['mean'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {results['loss_dt']['mean']:.4f}\")\n",
    "axes[1].axvline(results['loss_dt']['median'], color='green', linestyle='--', linewidth=2, label=f\"Median: {results['loss_dt']['median']:.4f}\")\n",
    "axes[1].set_xlabel('Time-to-Event Loss')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of DT Loss Across Batches')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Total Loss\n",
    "axes[2].hist(results['batch_losses_total'], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
    "axes[2].axvline(results['loss_total']['mean'], color='red', linestyle='--', linewidth=2, label=f\"Mean: {results['loss_total']['mean']:.4f}\")\n",
    "axes[2].axvline(results['loss_total']['median'], color='green', linestyle='--', linewidth=2, label=f\"Median: {results['loss_total']['median']:.4f}\")\n",
    "axes[2].set_xlabel('Total Loss (CE + DT)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Distribution of Total Loss Across Batches')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_evaluation_loss_distributions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved plot to 'test_evaluation_loss_distributions.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss trends across batches\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "batch_indices = np.arange(eval_iters)\n",
    "ax.plot(batch_indices, results['batch_losses_ce'], alpha=0.6, label='CE Loss', linewidth=1)\n",
    "ax.plot(batch_indices, results['batch_losses_dt'], alpha=0.6, label='DT Loss', linewidth=1)\n",
    "ax.plot(batch_indices, results['batch_losses_total'], alpha=0.6, label='Total Loss', linewidth=1)\n",
    "\n",
    "# Add rolling average\n",
    "window = min(10, eval_iters // 10)\n",
    "if window > 1:\n",
    "    from scipy.ndimage import uniform_filter1d\n",
    "    ax.plot(batch_indices, uniform_filter1d(results['batch_losses_total'], window), \n",
    "            color='red', linewidth=2, label=f'Total Loss (MA-{window})')\n",
    "\n",
    "ax.set_xlabel('Batch Index')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Values Across Test Batches')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_evaluation_loss_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved plot to 'test_evaluation_loss_trends.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary dictionary\n",
    "summary = {\n",
    "    'checkpoint': checkpoint_path,\n",
    "    'training_iteration': checkpoint['iter_num'],\n",
    "    'best_val_loss': checkpoint['best_val_loss'],\n",
    "    'test_batches': eval_iters,\n",
    "    'batch_size': batch_size,\n",
    "    'test_samples_approx': eval_iters * batch_size,\n",
    "    'ce_loss_mean': results['loss_ce']['mean'],\n",
    "    'ce_loss_std': results['loss_ce']['std'],\n",
    "    'ce_loss_median': results['loss_ce']['median'],\n",
    "    'dt_loss_mean': results['loss_dt']['mean'],\n",
    "    'dt_loss_std': results['loss_dt']['std'],\n",
    "    'dt_loss_median': results['loss_dt']['median'],\n",
    "    'total_loss_mean': results['loss_total']['mean'],\n",
    "    'total_loss_std': results['loss_total']['std'],\n",
    "    'total_loss_median': results['loss_total']['median'],\n",
    "    'perplexity': results['perplexity'],\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "import json\n",
    "output_file = 'test_evaluation_results.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f\"Saved results summary to '{output_file}'\")\n",
    "\n",
    "# Save detailed batch results to CSV\n",
    "batch_results_df = pd.DataFrame({\n",
    "    'batch_idx': np.arange(eval_iters),\n",
    "    'loss_ce': results['batch_losses_ce'],\n",
    "    'loss_dt': results['batch_losses_dt'],\n",
    "    'loss_total': results['batch_losses_total'],\n",
    "})\n",
    "batch_results_file = 'test_evaluation_batch_results.csv'\n",
    "batch_results_df.to_csv(batch_results_file, index=False)\n",
    "print(f\"Saved batch-level results to '{batch_results_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optional: Compare with Validation Set\n",
    "\n",
    "Compare test set performance with validation set to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation data\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint32, mode='r').reshape(-1, 3)\n",
    "val_p2i = get_p2i(val_data)\n",
    "\n",
    "print(f\"Evaluating on validation set...\")\n",
    "val_results = evaluate_test_set(\n",
    "    model=model,\n",
    "    test_data=val_data,\n",
    "    test_p2i=val_p2i,\n",
    "    eval_iters=eval_iters,\n",
    "    batch_size=batch_size,\n",
    "    block_size=block_size,\n",
    "    device=device,\n",
    "    no_event_token_rate=no_event_token_rate\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARISON: Test vs Validation\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nCE Loss:\")\n",
    "print(f\"  Test:       {results['loss_ce']['mean']:.4f}\")\n",
    "print(f\"  Validation: {val_results['loss_ce']['mean']:.4f}\")\n",
    "print(f\"  Difference: {results['loss_ce']['mean'] - val_results['loss_ce']['mean']:.4f}\")\n",
    "\n",
    "print(f\"\\nDT Loss:\")\n",
    "print(f\"  Test:       {results['loss_dt']['mean']:.4f}\")\n",
    "print(f\"  Validation: {val_results['loss_dt']['mean']:.4f}\")\n",
    "print(f\"  Difference: {results['loss_dt']['mean'] - val_results['loss_dt']['mean']:.4f}\")\n",
    "\n",
    "print(f\"\\nTotal Loss:\")\n",
    "print(f\"  Test:       {results['loss_total']['mean']:.4f}\")\n",
    "print(f\"  Validation: {val_results['loss_total']['mean']:.4f}\")\n",
    "print(f\"  Difference: {results['loss_total']['mean'] - val_results['loss_total']['mean']:.4f}\")\n",
    "\n",
    "print(f\"\\nPerplexity:\")\n",
    "print(f\"  Test:       {results['perplexity']:.2f}\")\n",
    "print(f\"  Validation: {val_results['perplexity']:.2f}\")\n",
    "print(f\"  Difference: {results['perplexity'] - val_results['perplexity']:.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "metrics = ['CE Loss', 'DT Loss', 'Total Loss']\n",
    "test_values = [results['loss_ce']['mean'], results['loss_dt']['mean'], results['loss_total']['mean']]\n",
    "val_values = [val_results['loss_ce']['mean'], val_results['loss_dt']['mean'], val_results['loss_total']['mean']]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, test_values, width, label='Test', alpha=0.8)\n",
    "axes[0].bar(x + width/2, val_values, width, label='Validation', alpha=0.8)\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Test vs Validation Loss Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(['Test', 'Validation'], [results['perplexity'], val_results['perplexity']], alpha=0.8, color=['blue', 'orange'])\n",
    "axes[1].set_ylabel('Perplexity')\n",
    "axes[1].set_title('Test vs Validation Perplexity')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_vs_validation_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved comparison plot to 'test_vs_validation_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Optional: Sample Predictions\n",
    "\n",
    "Examine model predictions on a few test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a single batch for inspection\n",
    "ix = torch.randint(len(test_p2i), (5,))  # Sample 5 patients\n",
    "X, A, Y, B = get_batch(ix, test_data, test_p2i, \n",
    "                      block_size=block_size,\n",
    "                      device=device, \n",
    "                      select='left',\n",
    "                      no_event_token_rate=no_event_token_rate, \n",
    "                      cut_batch=True)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    logits, loss, att = model(X, A, Y, B, validation_loss_mode=True)\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "print(\"Sample Predictions (first patient in batch):\\n\")\n",
    "print(\"Position | Input Token | Input Age | True Next | Predicted | Top-3 Predictions\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "patient_idx = 0\n",
    "for pos in range(min(10, X.shape[1])):  # Show first 10 positions\n",
    "    if X[patient_idx, pos].item() == 0:  # Skip padding\n",
    "        continue\n",
    "    \n",
    "    input_token = X[patient_idx, pos].item()\n",
    "    input_age = A[patient_idx, pos].item() / 365.25  # Convert to years\n",
    "    true_next = Y[patient_idx, pos].item()\n",
    "    pred_next = predicted_tokens[patient_idx, pos].item()\n",
    "    \n",
    "    # Get top 3 predictions\n",
    "    top3_probs, top3_tokens = torch.topk(probs[patient_idx, pos], 3)\n",
    "    top3_str = \", \".join([f\"{tok.item()}({prob.item():.2%})\" for tok, prob in zip(top3_tokens, top3_probs)])\n",
    "    \n",
    "    match = \"✓\" if pred_next == true_next else \"✗\"\n",
    "    print(f\"{pos:8d} | {input_token:11d} | {input_age:9.1f} | {true_next:9d} | {pred_next:9d} {match} | {top3_str}\")\n",
    "\n",
    "print(\"\\nNote: Tokens are shifted by +1 (0 is padding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook evaluates the Delphi model following best practices from BERT and similar language models:\n",
    "\n",
    "1. **Multiple metrics**: CE loss, DT loss, combined loss, and perplexity\n",
    "2. **Statistical analysis**: Mean, std, median, min, max for robust evaluation\n",
    "3. **Visualization**: Loss distributions and trends\n",
    "4. **Comparison**: Test vs validation to detect overfitting\n",
    "5. **Reproducibility**: Results saved to JSON and CSV files\n",
    "\n",
    "**Key Metrics to Report:**\n",
    "- Test Loss (CE + DT combined)\n",
    "- Perplexity (measures model uncertainty)\n",
    "- Comparison with validation loss (to check generalization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
